{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ece95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2ddd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.defaults： {'lr': 1e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "optimizer.param_groups长度： 2\n",
      "optimizer.param_groups一个元素包含的键： dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'foreach', 'maximize', 'capturable', 'differentiable', 'fused'])\n"
     ]
    }
   ],
   "source": [
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.SELU(),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.Linear(in_features, in_features),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.SELU(),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "            nn.Linear(out_features, out_features),\n",
    "        )\n",
    "        self.out_fc = nn.Linear(out_features, out_features)\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        return self.out_fc(x)\n",
    "\n",
    "model = SimpleFNN(10, 5)\n",
    "# model.parameters()\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.fc1.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.fc2.parameters(), 'lr': 1e-4, 'weight_decay': 0.01},],\n",
    "    lr= 1e-5)\n",
    "\n",
    "print(\"optimizer.defaults：\", optimizer.defaults)\n",
    "print(\"optimizer.param_groups长度：\", len(optimizer.param_groups))\n",
    "print(\"optimizer.param_groups一个元素包含的键：\", optimizer.param_groups[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02700046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LambdaLR...\n",
      "Testing MultiplicativeLR...\n",
      "Testing StepLR...\n",
      "Testing MultiStepLR...\n",
      "Testing ConstantLR...\n",
      "Testing LinearLR...\n",
      "Testing ExponentialLR...\n",
      "Testing CosineAnnealingLR...\n",
      "Testing ReduceLROnPlateau...\n",
      "Testing CyclicLR...\n",
      "Testing OneCycleLR...\n",
      "Testing CosineAnnealingWarmRestarts...\n",
      "Learning rate curves saved as 'lr_schedulers.png'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.SELU(),\n",
    "            nn.BatchNorm1d(out_features),  # 改为 BatchNorm1d 以适应 2D 输入\n",
    "            nn.Linear(out_features, in_features),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.SELU(),\n",
    "            nn.BatchNorm1d(out_features),  # 改为 BatchNorm1d\n",
    "            nn.Linear(out_features, out_features),\n",
    "        )\n",
    "        self.out_fc = nn.Linear(out_features, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.out_fc(x)\n",
    "\n",
    "# 生成随机数据\n",
    "def generate_data(batch_size=32, in_features=10):\n",
    "    x = torch.randn(batch_size, in_features)\n",
    "    y = torch.randn(batch_size, in_features)\n",
    "    return x, y\n",
    "\n",
    "# 训练和记录学习率\n",
    "def train_and_record_lr(scheduler, scheduler_name, epochs=1000, steps_per_epoch=10):\n",
    "    lrs = []\n",
    "    model = SimpleFNN(in_features=10, out_features=10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if scheduler_name == \"CyclicLR\" or scheduler_name == \"OneCycleLR\":\n",
    "        # 基于步数的调度器\n",
    "        scheduler = init_scheduler(scheduler_name, optimizer, steps_per_epoch * epochs)\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(steps_per_epoch):\n",
    "                x, y = generate_data()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                lrs.append(scheduler.get_last_lr()[0])\n",
    "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "        # 基于指标的调度器\n",
    "        scheduler = init_scheduler(scheduler_name, optimizer)\n",
    "        for epoch in range(epochs):\n",
    "            x, y = generate_data()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 模拟验证损失（随机值）\n",
    "            val_loss = np.random.uniform(0.1, 1.0)\n",
    "            scheduler.step(val_loss)\n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "    else:\n",
    "        # 基于 epoch 的调度器\n",
    "        scheduler = init_scheduler(scheduler_name, optimizer)\n",
    "        for epoch in range(epochs):\n",
    "            x, y = generate_data()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "    \n",
    "    return lrs\n",
    "\n",
    "def init_scheduler(name, optimizer, total_steps=None):\n",
    "    if name == \"LambdaLR\":\n",
    "        return LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "    elif name == \"MultiplicativeLR\":\n",
    "        return MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9)\n",
    "    elif name == \"StepLR\":\n",
    "        return StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    elif name == \"MultiStepLR\":\n",
    "        return MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n",
    "    elif name == \"ConstantLR\":\n",
    "        return ConstantLR(optimizer, factor=0.1, total_iters=5)\n",
    "    elif name == \"LinearLR\":\n",
    "        return LinearLR(optimizer, start_factor=0.3, end_factor=1.0, total_iters=5)\n",
    "    elif name == \"ExponentialLR\":\n",
    "        return ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif name == \"CosineAnnealingLR\":\n",
    "        return CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    elif name == \"ReduceLROnPlateau\":\n",
    "        return ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif name == \"CyclicLR\":\n",
    "        return CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=total_steps//4)\n",
    "    elif name == \"OneCycleLR\":\n",
    "        return OneCycleLR(optimizer, max_lr=0.1, total_steps=total_steps, pct_start=0.3)\n",
    "    elif name == \"CosineAnnealingWarmRestarts\":\n",
    "        return CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0)\n",
    "\n",
    "def plot_lr_curves(schedulers, lr_histories, epochs=100):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (scheduler_name, lrs) in enumerate(zip(schedulers, lr_histories)):\n",
    "        plt.subplot(4, 3, i+1)\n",
    "        plt.plot(lrs, label=scheduler_name)\n",
    "        plt.title(scheduler_name)\n",
    "        plt.xlabel(\"Step\" if scheduler_name in [\"CyclicLR\", \"OneCycleLR\"] else \"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lr_schedulers.png\")\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schedulers = [\n",
    "        \"LambdaLR\", \"MultiplicativeLR\", \"StepLR\", \"MultiStepLR\", \"ConstantLR\",\n",
    "        \"LinearLR\", \"ExponentialLR\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\",\n",
    "        \"CyclicLR\", \"OneCycleLR\", \"CosineAnnealingWarmRestarts\"\n",
    "    ]\n",
    "    lr_histories = []\n",
    "    \n",
    "    for scheduler_name in schedulers:\n",
    "        print(f\"Testing {scheduler_name}...\")\n",
    "        lrs = train_and_record_lr(scheduler_name, scheduler_name, epochs=100, steps_per_epoch=10)\n",
    "        lr_histories.append(lrs)\n",
    "    \n",
    "    plot_lr_curves(schedulers, lr_histories)\n",
    "    print(\"Learning rate curves saved as 'lr_schedulers.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87223d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler.CosineAnnealingLR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
